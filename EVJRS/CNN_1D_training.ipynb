{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Linear, ReLU, Dropout, Conv2d, MaxPool2d\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.optim import AdamW\n",
    "\n",
    "import gurobipy as gb\n",
    "from gurobipy import GRB\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if GPU available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# set CUDA_VISIBLE_DEVICES=0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "interval = 5\n",
    "\n",
    "train_test_dir = os.path.join(os.getcwd(), f\"dataGeneration/preprocessed_data_{interval}\")\n",
    "\n",
    "X_train = np.load(os.path.join(train_test_dir, \"X_train.npy\"))\n",
    "X_val = np.load(os.path.join(train_test_dir, \"X_val.npy\"))\n",
    "\n",
    "y_train = np.load(os.path.join(train_test_dir, \"y_train.npy\"))\n",
    "y_val = np.load(os.path.join(train_test_dir, \"y_val.npy\"))\n",
    "\n",
    "index_train = np.load(os.path.join(train_test_dir, \"indices_train.npy\")).astype(\"int64\")\n",
    "index_val = np.load(os.path.join(train_test_dir, \"indices_val.npy\")).astype(\"int64\")\n",
    "\n",
    "solTime_val = np.load(os.path.join(train_test_dir, \"solTime_val.npy\"))\n",
    "objVal_val = np.load(os.path.join(train_test_dir, \"objVal_val.npy\"))\n",
    "schedule_val = np.load(os.path.join(train_test_dir, \"schedule_val.npy\")).astype(\"int32\")\n",
    "model_val = np.load(os.path.join(train_test_dir, \"model_val.npy\")).astype(\"int32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(719, 48, 169) (80, 48, 169)\n",
      "float64\n",
      "(719, 767300) (80, 767300)\n",
      "float64\n",
      "(719, 767300) (80, 767300)\n",
      "int64\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_val.shape)\n",
    "print(X_train.dtype)\n",
    "print(y_train.shape, y_val.shape)\n",
    "print(y_train.dtype)\n",
    "print(index_train.shape, index_val.shape)\n",
    "print(index_train.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the CNN network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(719, 169, 48) (80, 169, 48)\n",
      "169 48 767300\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val = np.transpose(X_train, (0,2,1)), np.transpose(X_val, (0,2,1))\n",
    "print(X_train.shape, X_val.shape)\n",
    "\n",
    "in_channels = X_train.shape[1]\n",
    "col = X_train.shape[2]\n",
    "\n",
    "out_channels = y_train[0].size\n",
    "\n",
    "print(in_channels, col, out_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "\n",
    "        self.hidden_ch = 64\n",
    "        self.dp = 0.1\n",
    "\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, self.hidden_ch, 11, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dp),\n",
    "            nn.MaxPool1d(5, stride=1, padding=0),\n",
    "\n",
    "            nn.Conv1d(self.hidden_ch, self.hidden_ch*2, 7, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dp),\n",
    "            nn.MaxPool1d(5, stride=1, padding=0),\n",
    "\n",
    "            nn.Conv1d(self.hidden_ch*2, self.hidden_ch, 3, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dp),\n",
    "            nn.MaxPool1d(5, stride=1, padding=0),\n",
    "        )\n",
    "        \n",
    "        n_channels = self.feature_extractor(torch.zeros(1, in_channels, col)).size(-1)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.MaxPool1d(n_channels), # GAP\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.hidden_ch, self.hidden_ch*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dp),\n",
    "            nn.Linear(self.hidden_ch*2, self.hidden_ch*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dp),\n",
    "            nn.Linear(self.hidden_ch*2, out_channels),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        out = self.classifier(features)\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "        'batch_size' : 8, # Num samples to average over for gradient updates\n",
    "        'EPOCHS' : 150, # Num times to iterate over the entire dataset\n",
    "        'LEARNING_RATE' : 5e-4, # Learning rate for the optimizer\n",
    "        'WEIGHT_DECAY' : 1e-4, # Weight decay parameter for the Adam optimizer\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class coordinationDataset(TensorDataset):\n",
    "    def __init__(self, X, y):\n",
    "        super(coordinationDataset, self).__init__()\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        X = self.X[index]\n",
    "        y = self.y[index]\n",
    "        \n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "        y_tensor = torch.round(torch.tensor(y, dtype=torch.float32))\n",
    "\n",
    "        return X_tensor, y_tensor\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = coordinationDataset(X_train, y_train)\n",
    "val_dataset = coordinationDataset(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NeuralNetwork()\n",
    "\n",
    "batch_size = config['batch_size']\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=config[\"LEARNING_RATE\"])\n",
    "total_steps = len(train_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define custom loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asymmetric_loss(predict, target, gamma_neg=0.3, gamma_pos=0, clip=0.0, eps=1e-8, disable_torch_grad_focal_loss=True):\n",
    "\n",
    "    \"\"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: input logits\n",
    "    y: targets (multi-label binarized vector)\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculating Probabilities\n",
    "    x_sigmoid = predict\n",
    "    xs_pos = x_sigmoid\n",
    "    xs_neg = 1 - x_sigmoid\n",
    "\n",
    "    # Asymmetric Clipping\n",
    "    if clip is not None and clip > 0:\n",
    "        xs_neg = (xs_neg + clip).clamp(max=1)\n",
    "\n",
    "    # Basic CE calculation\n",
    "    los_pos = target * torch.log(xs_pos.clamp(min=eps))\n",
    "    los_neg = (1 - target) * torch.log(xs_neg.clamp(min=eps))\n",
    "    loss = los_pos + los_neg\n",
    "\n",
    "    # Asymmetric Focusing\n",
    "    if gamma_neg > 0 or gamma_pos > 0:\n",
    "        if disable_torch_grad_focal_loss:\n",
    "            torch.set_grad_enabled(False)\n",
    "        pt0 = xs_pos * target\n",
    "        pt1 = xs_neg * (1 - target)  # pt = p if t > 0 else 1-p\n",
    "        pt = pt0 + pt1\n",
    "        one_sided_gamma = gamma_pos * target + gamma_neg * (1 - target)\n",
    "        one_sided_w = torch.pow(1 - pt, one_sided_gamma)\n",
    "        if disable_torch_grad_focal_loss:\n",
    "            torch.set_grad_enabled(True)\n",
    "        loss *= one_sided_w\n",
    "\n",
    "    return -loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start of Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 0.146\n",
      "Epoch 1 val loss: 0.022\n",
      "Epoch 2 loss: 0.015\n",
      "Epoch 2 val loss: 0.015\n",
      "min loss:  0.02185544278472662\n",
      "Model saved\n",
      "Epoch 3 loss: 0.014\n",
      "Epoch 3 val loss: 0.014\n",
      "min loss:  0.014799443259835243\n",
      "Model saved\n",
      "Epoch 4 loss: 0.014\n",
      "Epoch 4 val loss: 0.014\n",
      "min loss:  0.014486995339393616\n",
      "Model saved\n",
      "Epoch 5 loss: 0.014\n",
      "Epoch 5 val loss: 0.014\n",
      "min loss:  0.014416238479316234\n",
      "Model saved\n",
      "Epoch 6 loss: 0.014\n",
      "Epoch 6 val loss: 0.014\n",
      "min loss:  0.014262507017701865\n",
      "Epoch 7 loss: 0.014\n",
      "Epoch 7 val loss: 0.014\n",
      "min loss:  0.014262507017701865\n",
      "Epoch 8 loss: 0.014\n",
      "Epoch 8 val loss: 0.014\n",
      "min loss:  0.014262507017701865\n",
      "Model saved\n",
      "Epoch 9 loss: 0.014\n",
      "Epoch 9 val loss: 0.014\n",
      "min loss:  0.014204000681638717\n",
      "Epoch 10 loss: 0.014\n",
      "Epoch 10 val loss: 0.014\n",
      "min loss:  0.014204000681638717\n",
      "Model saved\n",
      "Epoch 11 loss: 0.014\n",
      "Epoch 11 val loss: 0.014\n",
      "min loss:  0.014135484676808118\n",
      "Epoch 12 loss: 0.014\n",
      "Epoch 12 val loss: 0.014\n",
      "min loss:  0.014135484676808118\n",
      "Epoch 13 loss: 0.014\n",
      "Epoch 13 val loss: 0.014\n",
      "min loss:  0.014135484676808118\n",
      "Model saved\n",
      "Epoch 14 loss: 0.014\n",
      "Epoch 14 val loss: 0.014\n",
      "min loss:  0.014118548389524222\n",
      "Epoch 15 loss: 0.014\n",
      "Epoch 15 val loss: 0.014\n",
      "min loss:  0.014118548389524222\n",
      "Model saved\n",
      "Epoch 16 loss: 0.014\n",
      "Epoch 16 val loss: 0.014\n",
      "min loss:  0.014118212461471557\n",
      "Model saved\n",
      "Epoch 17 loss: 0.013\n",
      "Epoch 17 val loss: 0.014\n",
      "min loss:  0.014008601754903793\n",
      "Model saved\n",
      "Epoch 18 loss: 0.013\n",
      "Epoch 18 val loss: 0.014\n",
      "min loss:  0.013882400840520859\n",
      "Model saved\n",
      "Epoch 19 loss: 0.013\n",
      "Epoch 19 val loss: 0.014\n",
      "min loss:  0.013815902452915908\n",
      "Model saved\n",
      "Epoch 20 loss: 0.013\n",
      "Epoch 20 val loss: 0.014\n",
      "min loss:  0.013643497973680497\n",
      "Model saved\n",
      "Epoch 21 loss: 0.013\n",
      "Epoch 21 val loss: 0.014\n",
      "min loss:  0.013564473856240511\n",
      "Epoch 22 loss: 0.013\n",
      "Epoch 22 val loss: 0.014\n",
      "min loss:  0.013564473856240511\n",
      "Epoch 23 loss: 0.013\n",
      "Epoch 23 val loss: 0.014\n",
      "min loss:  0.013564473856240511\n",
      "Epoch 24 loss: 0.013\n",
      "Epoch 24 val loss: 0.014\n",
      "min loss:  0.013564473856240511\n",
      "Epoch 25 loss: 0.013\n",
      "Epoch 25 val loss: 0.014\n",
      "min loss:  0.013564473856240511\n",
      "Model saved\n",
      "Epoch 26 loss: 0.013\n",
      "Epoch 26 val loss: 0.014\n",
      "min loss:  0.013539365865290165\n",
      "Epoch 27 loss: 0.013\n",
      "Epoch 27 val loss: 0.014\n",
      "min loss:  0.013539365865290165\n",
      "Model saved\n",
      "Epoch 28 loss: 0.013\n",
      "Epoch 28 val loss: 0.014\n",
      "min loss:  0.013529787864536047\n",
      "Epoch 29 loss: 0.013\n",
      "Epoch 29 val loss: 0.013\n",
      "min loss:  0.013529787864536047\n",
      "Model saved\n",
      "Epoch 30 loss: 0.013\n",
      "Epoch 30 val loss: 0.013\n",
      "min loss:  0.013369664456695319\n",
      "Epoch 31 loss: 0.013\n",
      "Epoch 31 val loss: 0.013\n",
      "min loss:  0.013369664456695319\n",
      "Epoch 32 loss: 0.012\n",
      "Epoch 32 val loss: 0.013\n",
      "min loss:  0.013369664456695319\n",
      "Model saved\n",
      "Epoch 33 loss: 0.012\n",
      "Epoch 33 val loss: 0.013\n",
      "min loss:  0.013201776146888732\n",
      "Model saved\n",
      "Epoch 34 loss: 0.012\n",
      "Epoch 34 val loss: 0.013\n",
      "min loss:  0.013051866181194782\n",
      "Epoch 35 loss: 0.012\n",
      "Epoch 35 val loss: 0.013\n",
      "min loss:  0.013051866181194782\n",
      "Model saved\n",
      "Epoch 36 loss: 0.012\n",
      "Epoch 36 val loss: 0.013\n",
      "min loss:  0.012925452645868063\n",
      "Epoch 37 loss: 0.012\n",
      "Epoch 37 val loss: 0.013\n",
      "min loss:  0.012925452645868063\n",
      "Epoch 38 loss: 0.012\n",
      "Epoch 38 val loss: 0.013\n",
      "min loss:  0.012925452645868063\n",
      "Model saved\n",
      "Epoch 39 loss: 0.012\n",
      "Epoch 39 val loss: 0.013\n",
      "min loss:  0.012867560610175133\n",
      "Model saved\n",
      "Epoch 40 loss: 0.012\n",
      "Epoch 40 val loss: 0.013\n",
      "min loss:  0.012763195298612117\n",
      "Model saved\n",
      "Epoch 41 loss: 0.012\n",
      "Epoch 41 val loss: 0.013\n",
      "min loss:  0.012690369877964259\n",
      "Model saved\n",
      "Epoch 42 loss: 0.012\n",
      "Epoch 42 val loss: 0.012\n",
      "min loss:  0.012546836864203215\n",
      "Model saved\n",
      "Epoch 43 loss: 0.012\n",
      "Epoch 43 val loss: 0.012\n",
      "min loss:  0.012449496146291494\n",
      "Epoch 44 loss: 0.012\n",
      "Epoch 44 val loss: 0.012\n",
      "min loss:  0.012449496146291494\n",
      "Model saved\n",
      "Epoch 45 loss: 0.011\n",
      "Epoch 45 val loss: 0.012\n",
      "min loss:  0.012298065982759\n",
      "Model saved\n",
      "Epoch 46 loss: 0.011\n",
      "Epoch 46 val loss: 0.012\n",
      "min loss:  0.012234274111688138\n",
      "Model saved\n",
      "Epoch 47 loss: 0.011\n",
      "Epoch 47 val loss: 0.012\n",
      "min loss:  0.012026744056493044\n",
      "Model saved\n",
      "Epoch 48 loss: 0.011\n",
      "Epoch 48 val loss: 0.012\n",
      "min loss:  0.011870579421520233\n",
      "Model saved\n",
      "Epoch 49 loss: 0.011\n",
      "Epoch 49 val loss: 0.011\n",
      "min loss:  0.01155910948291421\n",
      "Model saved\n",
      "Epoch 50 loss: 0.010\n",
      "Epoch 50 val loss: 0.011\n",
      "min loss:  0.011340264789760112\n",
      "Model saved\n",
      "Epoch 51 loss: 0.010\n",
      "Epoch 51 val loss: 0.011\n",
      "min loss:  0.010972184501588345\n",
      "Model saved\n",
      "Epoch 52 loss: 0.010\n",
      "Epoch 52 val loss: 0.010\n",
      "min loss:  0.010614473093301057\n",
      "Model saved\n",
      "Epoch 53 loss: 0.009\n",
      "Epoch 53 val loss: 0.010\n",
      "min loss:  0.010381793417036534\n",
      "Model saved\n",
      "Epoch 54 loss: 0.009\n",
      "Epoch 54 val loss: 0.010\n",
      "min loss:  0.010102677997201681\n",
      "Model saved\n",
      "Epoch 55 loss: 0.008\n",
      "Epoch 55 val loss: 0.010\n",
      "min loss:  0.009780263062566519\n",
      "Model saved\n",
      "Epoch 56 loss: 0.008\n",
      "Epoch 56 val loss: 0.009\n",
      "min loss:  0.009515892248600721\n",
      "Model saved\n",
      "Epoch 57 loss: 0.008\n",
      "Epoch 57 val loss: 0.009\n",
      "min loss:  0.009272181056439876\n",
      "Model saved\n",
      "Epoch 58 loss: 0.008\n",
      "Epoch 58 val loss: 0.009\n",
      "min loss:  0.009031556313857436\n",
      "Model saved\n",
      "Epoch 59 loss: 0.007\n",
      "Epoch 59 val loss: 0.009\n",
      "min loss:  0.008747061574831605\n",
      "Model saved\n",
      "Epoch 60 loss: 0.007\n",
      "Epoch 60 val loss: 0.008\n",
      "min loss:  0.008580097183585166\n",
      "Model saved\n",
      "Epoch 61 loss: 0.007\n",
      "Epoch 61 val loss: 0.008\n",
      "min loss:  0.008364236447960138\n",
      "Model saved\n",
      "Epoch 62 loss: 0.007\n",
      "Epoch 62 val loss: 0.008\n",
      "min loss:  0.008220497844740748\n",
      "Model saved\n",
      "Epoch 63 loss: 0.007\n",
      "Epoch 63 val loss: 0.008\n",
      "min loss:  0.008164273761212825\n",
      "Model saved\n",
      "Epoch 64 loss: 0.006\n",
      "Epoch 64 val loss: 0.008\n",
      "min loss:  0.00791690992191434\n",
      "Model saved\n",
      "Epoch 65 loss: 0.006\n",
      "Epoch 65 val loss: 0.008\n",
      "min loss:  0.007805379433557391\n",
      "Model saved\n",
      "Epoch 66 loss: 0.006\n",
      "Epoch 66 val loss: 0.008\n",
      "min loss:  0.00773980924859643\n",
      "Model saved\n",
      "Epoch 67 loss: 0.006\n",
      "Epoch 67 val loss: 0.008\n",
      "min loss:  0.007652216544374824\n",
      "Model saved\n",
      "Epoch 68 loss: 0.006\n",
      "Epoch 68 val loss: 0.008\n",
      "min loss:  0.007582109281793236\n",
      "Model saved\n",
      "Epoch 69 loss: 0.006\n",
      "Epoch 69 val loss: 0.007\n",
      "min loss:  0.007563151279464364\n",
      "Model saved\n",
      "Epoch 70 loss: 0.006\n",
      "Epoch 70 val loss: 0.007\n",
      "min loss:  0.007498857052996755\n",
      "Model saved\n",
      "Epoch 71 loss: 0.006\n",
      "Epoch 71 val loss: 0.007\n",
      "min loss:  0.007491991855204105\n",
      "Model saved\n",
      "Epoch 72 loss: 0.006\n",
      "Epoch 72 val loss: 0.007\n",
      "min loss:  0.007450115261599421\n",
      "Model saved\n",
      "Epoch 73 loss: 0.006\n",
      "Epoch 73 val loss: 0.007\n",
      "min loss:  0.00743118803948164\n",
      "Epoch 74 loss: 0.006\n",
      "Epoch 74 val loss: 0.007\n",
      "min loss:  0.00743118803948164\n",
      "Model saved\n",
      "Epoch 75 loss: 0.006\n",
      "Epoch 75 val loss: 0.007\n",
      "min loss:  0.0074179467745125295\n",
      "Epoch 76 loss: 0.006\n",
      "Epoch 76 val loss: 0.007\n",
      "min loss:  0.0074179467745125295\n",
      "Model saved\n",
      "Epoch 77 loss: 0.006\n",
      "Epoch 77 val loss: 0.007\n",
      "min loss:  0.00739257843233645\n",
      "Model saved\n",
      "Epoch 78 loss: 0.006\n",
      "Epoch 78 val loss: 0.007\n",
      "min loss:  0.007346230698749423\n",
      "Model saved\n",
      "Epoch 79 loss: 0.006\n",
      "Epoch 79 val loss: 0.007\n",
      "min loss:  0.007345672603696585\n",
      "Model saved\n",
      "Epoch 80 loss: 0.006\n",
      "Epoch 80 val loss: 0.007\n",
      "min loss:  0.007322709308937192\n",
      "Epoch 81 loss: 0.006\n",
      "Epoch 81 val loss: 0.007\n",
      "min loss:  0.007322709308937192\n",
      "Epoch 82 loss: 0.006\n",
      "Epoch 82 val loss: 0.007\n",
      "min loss:  0.007322709308937192\n",
      "Model saved\n",
      "Epoch 83 loss: 0.006\n",
      "Epoch 83 val loss: 0.007\n",
      "min loss:  0.007293602451682091\n",
      "Epoch 84 loss: 0.006\n",
      "Epoch 84 val loss: 0.007\n",
      "min loss:  0.007293602451682091\n",
      "Epoch 85 loss: 0.006\n",
      "Epoch 85 val loss: 0.007\n",
      "min loss:  0.007293602451682091\n",
      "Epoch 86 loss: 0.006\n",
      "Epoch 86 val loss: 0.007\n",
      "min loss:  0.007293602451682091\n",
      "Epoch 87 loss: 0.006\n",
      "Epoch 87 val loss: 0.007\n",
      "min loss:  0.007293602451682091\n",
      "Epoch 88 loss: 0.006\n",
      "Epoch 88 val loss: 0.007\n",
      "min loss:  0.007293602451682091\n",
      "Epoch 89 loss: 0.006\n",
      "Epoch 89 val loss: 0.007\n",
      "min loss:  0.007293602451682091\n",
      "Model saved\n",
      "Epoch 90 loss: 0.006\n",
      "Epoch 90 val loss: 0.007\n",
      "min loss:  0.007286786707118154\n",
      "Epoch 91 loss: 0.006\n",
      "Epoch 91 val loss: 0.007\n",
      "min loss:  0.007286786707118154\n",
      "Epoch 92 loss: 0.005\n",
      "Epoch 92 val loss: 0.007\n",
      "min loss:  0.007286786707118154\n",
      "Epoch 93 loss: 0.005\n",
      "Epoch 93 val loss: 0.007\n",
      "min loss:  0.007286786707118154\n",
      "Epoch 94 loss: 0.005\n",
      "Epoch 94 val loss: 0.007\n",
      "min loss:  0.007286786707118154\n",
      "Epoch 95 loss: 0.005\n",
      "Epoch 95 val loss: 0.007\n",
      "min loss:  0.007286786707118154\n",
      "Epoch 96 loss: 0.005\n",
      "Epoch 96 val loss: 0.007\n",
      "min loss:  0.007286786707118154\n",
      "Epoch 97 loss: 0.005\n",
      "Epoch 97 val loss: 0.007\n",
      "min loss:  0.007286786707118154\n",
      "Epoch 98 loss: 0.005\n",
      "Epoch 98 val loss: 0.007\n",
      "min loss:  0.007286786707118154\n",
      "Epoch 99 loss: 0.005\n",
      "Epoch 99 val loss: 0.007\n",
      "min loss:  0.007286786707118154\n",
      "Epoch 100 loss: 0.005\n",
      "Epoch 100 val loss: 0.007\n",
      "min loss:  0.007286786707118154\n",
      "Epoch 101 loss: 0.005\n",
      "Epoch 101 val loss: 0.007\n",
      "min loss:  0.007286786707118154\n",
      "Epoch 102 loss: 0.005\n",
      "Epoch 102 val loss: 0.007\n",
      "min loss:  0.007286786707118154\n",
      "Epoch 103 loss: 0.006\n",
      "Epoch 103 val loss: 0.007\n",
      "min loss:  0.007286786707118154\n",
      "Epoch 104 loss: 0.005\n",
      "Epoch 104 val loss: 0.007\n",
      "min loss:  0.007286786707118154\n",
      "Epoch 105 loss: 0.005\n",
      "Epoch 105 val loss: 0.007\n",
      "min loss:  0.007286786707118154\n",
      "Epoch 106 loss: 0.005\n",
      "Epoch 106 val loss: 0.007\n",
      "min loss:  0.007286786707118154\n",
      "Epoch 107 loss: 0.005\n",
      "Epoch 107 val loss: 0.007\n",
      "min loss:  0.007286786707118154\n",
      "Epoch 108 loss: 0.005\n",
      "Epoch 108 val loss: 0.007\n",
      "min loss:  0.007286786707118154\n",
      "Epoch 109 loss: 0.005\n",
      "Epoch 109 val loss: 0.007\n",
      "min loss:  0.007286786707118154\n",
      "Epoch 110 loss: 0.005\n",
      "Epoch 110 val loss: 0.007\n",
      "min loss:  0.007286786707118154\n",
      "Epoch 111 loss: 0.005\n",
      "Epoch 111 val loss: 0.007\n",
      "min loss:  0.007286786707118154\n",
      "Epoch 112 loss: 0.005\n",
      "Epoch 112 val loss: 0.007\n",
      "min loss:  0.007286786707118154\n",
      "Epoch 113 loss: 0.005\n",
      "Epoch 113 val loss: 0.007\n",
      "min loss:  0.007286786707118154\n",
      "Epoch 114 loss: 0.005\n",
      "Epoch 114 val loss: 0.007\n",
      "min loss:  0.007286786707118154\n",
      "Epoch 115 loss: 0.005\n",
      "Epoch 115 val loss: 0.007\n",
      "min loss:  0.007286786707118154\n",
      "Epoch 116 loss: 0.005\n",
      "Epoch 116 val loss: 0.008\n",
      "min loss:  0.007286786707118154\n",
      "Epoch 117 loss: 0.005\n",
      "Epoch 117 val loss: 0.007\n",
      "min loss:  0.007286786707118154\n",
      "Epoch 118 loss: 0.005\n",
      "Epoch 118 val loss: 0.007\n",
      "min loss:  0.007286786707118154\n",
      "Epoch 119 loss: 0.005\n",
      "Epoch 119 val loss: 0.007\n",
      "min loss:  0.007286786707118154\n",
      "Epoch 120 loss: 0.005\n",
      "Epoch 120 val loss: 0.007\n",
      "min loss:  0.007286786707118154\n",
      "Epoch 121 loss: 0.005\n",
      "Epoch 121 val loss: 0.008\n",
      "min loss:  0.007286786707118154\n",
      "Epoch 122 loss: 0.005\n",
      "Epoch 122 val loss: 0.008\n",
      "min loss:  0.007286786707118154\n",
      "Epoch 123 loss: 0.005\n",
      "Epoch 123 val loss: 0.008\n",
      "min loss:  0.007286786707118154\n",
      "Epoch 124 loss: 0.005\n",
      "Epoch 124 val loss: 0.008\n",
      "min loss:  0.007286786707118154\n",
      "Epoch 125 loss: 0.005\n",
      "Epoch 125 val loss: 0.008\n",
      "min loss:  0.007286786707118154\n",
      "Epoch 126 loss: 0.005\n",
      "Epoch 126 val loss: 0.008\n",
      "min loss:  0.007286786707118154\n",
      "Epoch 127 loss: 0.005\n",
      "Epoch 127 val loss: 0.008\n",
      "min loss:  0.007286786707118154\n",
      "Epoch 128 loss: 0.005\n",
      "Epoch 128 val loss: 0.008\n",
      "min loss:  0.007286786707118154\n",
      "Epoch 129 loss: 0.005\n",
      "Epoch 129 val loss: 0.008\n",
      "min loss:  0.007286786707118154\n",
      "Epoch 130 loss: 0.005\n",
      "Epoch 130 val loss: 0.008\n",
      "min loss:  0.007286786707118154\n",
      "Epoch 131 loss: 0.005\n",
      "Epoch 131 val loss: 0.008\n",
      "min loss:  0.007286786707118154\n",
      "Epoch 132 loss: 0.005\n",
      "Epoch 132 val loss: 0.008\n",
      "min loss:  0.007286786707118154\n",
      "Epoch 133 loss: 0.005\n",
      "Epoch 133 val loss: 0.008\n",
      "min loss:  0.007286786707118154\n",
      "Epoch 134 loss: 0.005\n",
      "Epoch 134 val loss: 0.008\n",
      "min loss:  0.007286786707118154\n",
      "Epoch 135 loss: 0.005\n",
      "Epoch 135 val loss: 0.008\n",
      "min loss:  0.007286786707118154\n",
      "Epoch 136 loss: 0.005\n",
      "Epoch 136 val loss: 0.008\n",
      "min loss:  0.007286786707118154\n",
      "Epoch 137 loss: 0.005\n",
      "Epoch 137 val loss: 0.008\n",
      "min loss:  0.007286786707118154\n",
      "Epoch 138 loss: 0.005\n",
      "Epoch 138 val loss: 0.008\n",
      "min loss:  0.007286786707118154\n",
      "Epoch 139 loss: 0.005\n",
      "Epoch 139 val loss: 0.008\n",
      "min loss:  0.007286786707118154\n",
      "Epoch 140 loss: 0.005\n",
      "Epoch 140 val loss: 0.008\n",
      "min loss:  0.007286786707118154\n",
      "Epoch 141 loss: 0.005\n",
      "Epoch 141 val loss: 0.008\n",
      "min loss:  0.007286786707118154\n",
      "Epoch 142 loss: 0.005\n",
      "Epoch 142 val loss: 0.008\n",
      "min loss:  0.007286786707118154\n",
      "Epoch 143 loss: 0.005\n",
      "Epoch 143 val loss: 0.008\n",
      "min loss:  0.007286786707118154\n",
      "Epoch 144 loss: 0.005\n",
      "Epoch 144 val loss: 0.008\n",
      "min loss:  0.007286786707118154\n",
      "Epoch 145 loss: 0.005\n",
      "Epoch 145 val loss: 0.008\n",
      "min loss:  0.007286786707118154\n",
      "Epoch 146 loss: 0.005\n",
      "Epoch 146 val loss: 0.008\n",
      "min loss:  0.007286786707118154\n",
      "Epoch 147 loss: 0.005\n",
      "Epoch 147 val loss: 0.008\n",
      "min loss:  0.007286786707118154\n",
      "Epoch 148 loss: 0.005\n",
      "Epoch 148 val loss: 0.008\n",
      "min loss:  0.007286786707118154\n",
      "Epoch 149 loss: 0.005\n",
      "Epoch 149 val loss: 0.008\n",
      "min loss:  0.007286786707118154\n",
      "Epoch 150 loss: 0.005\n",
      "Epoch 150 val loss: 0.008\n",
      "min loss:  0.007286786707118154\n"
     ]
    }
   ],
   "source": [
    "loss_list = []\n",
    "loss_list_val = []\n",
    "\n",
    "for epoch in range(config[\"EPOCHS\"]):\n",
    "    running_loss = 0.0\n",
    "    running_loss_val = 0.0\n",
    "    \n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        # loss_fn = nn.BCELoss()     \n",
    "        loss = asymmetric_loss(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print('Epoch %d loss: %.3f' % (epoch + 1, running_loss / len(train_loader)))\n",
    "\n",
    "    for j, data in enumerate(valid_loader):\n",
    "        net.eval()\n",
    "        inputs, labels = data\n",
    "        \n",
    "        inputs, labels = inputs.to(device), labels.to(device)        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        # loss_fn = nn.BCELoss()  \n",
    "        loss = asymmetric_loss(outputs, labels)\n",
    "        running_loss_val += loss.item()\n",
    "    print('Epoch %d val loss: %.3f' % (epoch + 1, running_loss_val / len(valid_loader)))\n",
    "\n",
    "    if len(loss_list_val) > 0:\n",
    "        print(\"min loss: \", min(loss_list_val))\n",
    "        if (running_loss_val / len(valid_loader)) < min(loss_list_val):\n",
    "            torch.save(net.state_dict(), os.path.join(os.getcwd(), f\"ML_Model/CNN_1D_coordination_{interval}.pth\"))\n",
    "            print(\"Model saved\")\n",
    "    \n",
    "    loss_list.append(running_loss / len(train_loader))\n",
    "    loss_list_val.append(running_loss_val / len(valid_loader))\n",
    "    \n",
    "    # if training loss is lower than previous loss, save the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x251a32e9130>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_list)\n",
    "plt.plot(loss_list_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "net = NeuralNetwork()\n",
    "net.load_state_dict(torch.load(os.path.join(os.getcwd(), f\"ML_Model/CNN_1D_coordination_{interval}.pth\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test number of feasible solutions\n",
    "# test the model on the test set\n",
    "net.eval()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing of bit accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thres = 0.5\n",
    "\n",
    "one_accuracy = []\n",
    "zero_accuracy = []\n",
    "bit_accuracy = []\n",
    "running_loss = 0\n",
    "mean_one = []\n",
    "mean_zero = []\n",
    "\n",
    "for j, data in enumerate(valid_loader):\n",
    "    \n",
    "    net.eval()\n",
    "    inputs, labels = data\n",
    "\n",
    "    inputs, labels = inputs.to(device), labels.to(device)       \n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(inputs)\n",
    "\n",
    "    loss_fn = nn.BCELoss()\n",
    "    loss = loss_fn(outputs, labels)\n",
    "    running_loss += loss.item()\n",
    "\n",
    "    # start testing\n",
    "    outputs = (outputs).reshape(-1,)   \n",
    "    outputs_percent = outputs\n",
    "    outputs = torch.where(outputs >= thres, torch.ceil(outputs), torch.floor(outputs)).reshape(-1,)\n",
    "    # outputs = torch.round(outputs)\n",
    "    labels = labels.reshape(-1,)\n",
    "\n",
    "    one_labels = torch.where(labels == 1)\n",
    "    zero_labels = torch.where(labels == 0)\n",
    "    \n",
    "    one_outputs = outputs[one_labels]\n",
    "    zero_outputs = outputs[zero_labels]\n",
    "\n",
    "    one_acc = 1 - torch.sum(torch.abs(1 - one_outputs)) / one_outputs.shape[0] # 1 minus percentage of error\n",
    "    zero_acc = 1 - torch.sum(torch.abs(0 - zero_outputs)) / zero_outputs.shape[0]\n",
    "    bit_acc = 1 - torch.sum(torch.abs(outputs - labels)) / labels.shape[0]\n",
    "\n",
    "    one_accuracy.append(one_acc.cpu().detach().numpy())\n",
    "    zero_accuracy.append(zero_acc.cpu().detach().numpy())\n",
    "    bit_accuracy.append(bit_acc.cpu().detach().numpy())\n",
    "\n",
    "    # mean acc\n",
    "    id_1 = torch.where(outputs == 1)\n",
    "    id_0 = torch.where(outputs == 0)\n",
    "\n",
    "    p_1 = outputs_percent[id_1]\n",
    "    p_0 = outputs_percent[id_0]\n",
    "\n",
    "\n",
    "    y_1 = labels[id_1]\n",
    "    y_0 = labels[id_0]\n",
    "\n",
    "    y_1_1 = torch.where(y_1 == 1)\n",
    "    y_1_0 = torch.where(y_1 == 0)\n",
    "    y_0_1 = torch.where(y_0 == 1)\n",
    "    y_0_0 = torch.where(y_0 == 0)\n",
    "\n",
    "    avg_1 = torch.mean(torch.cat((p_1[y_1_1], torch.ones(y_1_0[0].shape[0]).to(device) - p_1[y_1_0])))\n",
    "    avg_0 = torch.mean(torch.cat((p_0[y_0_1], torch.ones(y_0_0[0].shape[0]).to(device) - p_0[y_0_0])))\n",
    "\n",
    "    # avg_1 = torch.mean(torch.cat((p_1[y_1_1],  p_1[y_1_0])))\n",
    "    # avg_0 = torch.mean(torch.cat((p_0[y_0_1], p_0[y_0_0])))\n",
    "\n",
    "    # avg_1 = torch.mean(p_1[y_1_1])\n",
    "    # avg_0 = torch.mean(p_0[y_0_1])\n",
    "\n",
    "    # avg_1 = torch.mean(p_1[y_1_0])\n",
    "    # avg_0 = torch.mean(p_0[y_0_1])\n",
    "\n",
    "    mean_one.append(avg_1.cpu().detach().numpy())\n",
    "    mean_zero.append(avg_0.cpu().detach().numpy())\n",
    "\n",
    "print(\"Average one bit accuracy\", np.mean(one_accuracy))\n",
    "print(\"Average zero bit accuracy\", np.mean(zero_accuracy))\n",
    "print(\"Average bit accuracy\", np.mean(bit_accuracy))\n",
    "print('Loss:', running_loss / len(valid_loader))\n",
    "print(np.mean(mean_one), np.mean(mean_zero))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for baseline solving speed (speed cap at 300 seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gurobi_env = gb.Env()\n",
    "gurobi_env.setParam(\"OutputFlag\", 0)\n",
    "    \n",
    "# loop through all test models and calculate average optimization time\n",
    "opt_time = []\n",
    "opt_val = []\n",
    "opt_ones = []\n",
    "for i, _ in enumerate(model_val):\n",
    "    \n",
    "    runtime = solTime_val[i]\n",
    "    obj = objVal_val[i]\n",
    "    \n",
    "    print(\"Optimization time for model \", i, \": \", runtime)\n",
    "    print(\"Optimization Value for model \", i, \": \", obj)\n",
    "    \n",
    "    opt_time.append(runtime)\n",
    "    opt_val.append(obj)\n",
    "\n",
    "    nbEV = np.max(schedule_val[i][:,0]) + 1\n",
    "    binary_vars = y_val[i]\n",
    "\n",
    "    opt_ones.append(np.count_nonzero(np.round(binary_vars)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_dict = {\n",
    "    \"opt_time\": opt_time,\n",
    "    \"opt_val\": opt_val,\n",
    "    \"opt_ones\": opt_ones\n",
    "}\n",
    "\n",
    "result_path = os.path.join(os.getcwd(), f\"Results\")\n",
    "with open(os.path.join(result_path, \"opt.pkl\"), 'wb') as f:\n",
    "    pickle.dump(opt_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_path = os.path.join(os.getcwd(), f\"Results\")\n",
    "with open(os.path.join(result_path, \"opt.pkl\"), 'rb') as f:\n",
    "    opt_dict = pickle.load(f)\n",
    "\n",
    "opt_time = opt_dict[\"opt_time\"]\n",
    "opt_val = opt_dict[\"opt_val\"]\n",
    "opt_ones = opt_dict[\"opt_ones\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten opt_time\n",
    "print(\"Average optimization time: \", np.mean(opt_time))\n",
    "opt_time_baseline = np.mean(opt_time)\n",
    "# flatten opt_time\n",
    "print(\"Average optimization value: \", np.mean(opt_val))\n",
    "opt_val_baseline = np.mean(opt_val)\n",
    "print(\"Number of ones: \", opt_ones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the optimization time for each instance in the test set if we use $\\color{lightblue}\\text{equality constraint}$ to find optimal solution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start testing equality constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in paramaeters\n",
    "data_dir = os.path.join(os.getcwd(), 'systemData')\n",
    "EV_routes = pd.read_csv(os.path.join(data_dir, 'EV_routes.csv')).to_numpy()\n",
    "bus_params = pd.read_csv(os.path.join(data_dir, 'bus_params.csv')).to_numpy()\n",
    "# EV_schedules = pd.read_csv(os.path.join(data_dir, 'EV_schedules.csv')).to_numpy().astype(\"int32\")\n",
    "\n",
    "nbScen, nbTime, nbEV, nbBus, nbRoute = 5, 48, 25, bus_params.shape[0], EV_routes.shape[0]\n",
    "traffic = np.zeros(nbTime-1)\n",
    "traffic[14:20] = 1      # from 7-10am \n",
    "traffic[32:40] = 1      # from 4-8pm\n",
    "\n",
    "charging_station = np.squeeze(pd.read_csv(os.path.join(data_dir, 'cs_params_variable.csv')).to_numpy())\n",
    "non_charging_station = np.array([i for i in range(nbBus) if i not in charging_station])\n",
    "nbCS = len(charging_station)\n",
    "print(nbCS)\n",
    "\n",
    "normal_nodes =  list(charging_station) + list(range(101,108))\n",
    "virtual_nodes = list(range(201,205))\n",
    "congest_nodes = list(range(301,324))\n",
    "\n",
    "always_arc, normal_arc, congest_arc = [], [], []\n",
    "\n",
    "for r in range(nbRoute):\n",
    "    if (EV_routes[r,1] in (normal_nodes+virtual_nodes)) and (EV_routes[r,2] in (normal_nodes+virtual_nodes)) and (EV_routes[r,1] != EV_routes[r,2]):\n",
    "        normal_arc.append(r)\n",
    "    elif (EV_routes[r,1] in (normal_nodes+virtual_nodes)) and (EV_routes[r,2] in congest_nodes):\n",
    "        congest_arc.append(r)\n",
    "    else:\n",
    "        always_arc.append(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "gurobi_env = gb.Env()\n",
    "gurobi_env.setParam(\"OutputFlag\", 0)\n",
    "\n",
    "# loop through all test models and calculate average optimization time\n",
    "opt_time_thres = []\n",
    "opt_val_thres = []\n",
    "opt_ones_thres = []\n",
    "opt_utilized_thres = []\n",
    "for i, x in enumerate(model_val):\n",
    "    \n",
    "    path = os.path.join(os.getcwd(), \"dataGeneration/model\")\n",
    "\n",
    "    model = gb.read(os.path.join(path, f\"coordination_{x}.mps\"), env=gurobi_env)\n",
    "    model.setParam(\"OutputFlag\", 0)\n",
    "    model.setParam(\"TimeLimit\", 30*60)\n",
    "\n",
    "    ################# timing the first code block #####################\n",
    "    start = time.time()\n",
    "\n",
    "    # deep learning prediciton\n",
    "    inputs = torch.tensor(np.expand_dims(val_dataset.X[i], axis=0), dtype=torch.float32) \n",
    "    inputs = inputs.to(device)    \n",
    "    outputs = net(inputs) \n",
    "\n",
    "    ################## end #########################\n",
    "\n",
    "    y_pred_binary =  (outputs).reshape(-1,).cpu().detach().numpy()\n",
    "    # y_pred_binary = y_val[i]\n",
    "\n",
    "    modelVars = model.getVars()\n",
    "\n",
    "    bin_id = []\n",
    "    ###### Build the index for the variables ######\n",
    "    for k in range(nbEV):\n",
    "        # for each EV get each attribute and order it based on EV number\n",
    "        for r in range(nbRoute):\n",
    "            for t in range(nbTime-1):\n",
    "                var = model.getVarByName(f\"EVArcStatus[{k},{r},{t}]\")\n",
    "\n",
    "                bin_id.append(var.index)\n",
    "\n",
    "    for k in range(nbEV):\n",
    "        for c in range(nbCS):\n",
    "            for t in range(nbTime):\n",
    "                var = model.getVarByName(f\"EVChargeStatus[{k},{c},{t}]\")\n",
    "\n",
    "                bin_id.append(var.index)\n",
    "                \n",
    "    for k in range(nbEV):\n",
    "        for c in range(nbCS):\n",
    "            for t in range(nbTime):\n",
    "                var = model.getVarByName(f\"EVDischargeStatus[{k},{c},{t}]\")\n",
    "\n",
    "                bin_id.append(var.index)\n",
    "    ################## end #########################\n",
    "\n",
    "    # bin_id = index_val[i]\n",
    "    # print(np.array(bin_id) - index_val[i])\n",
    "\n",
    "    # sys.exit()\n",
    "\n",
    "    one_threshold = (np.mean(mean_one))\n",
    "    zero_threshold = (1-np.mean(mean_zero))\n",
    "\n",
    "    # use equality constraint from here on\n",
    "    for j in range(len(y_pred_binary)):\n",
    "        if (y_pred_binary[j] >= one_threshold or y_pred_binary[j] <= zero_threshold):\n",
    "        # if (y_pred_binary[j] >= 0):\n",
    "            modelVars[bin_id[j]].setAttr(\"LB\", round(y_pred_binary[j]))\n",
    "            modelVars[bin_id[j]].setAttr(\"UB\", round(y_pred_binary[j]))\n",
    "            \n",
    "    end = time.time()\n",
    "    runtime1 = end - start\n",
    "\n",
    "    opt_ones_thres.append(np.count_nonzero(y_pred_binary >= one_threshold))\n",
    "    opt_utilized_thres.append(np.count_nonzero(y_pred_binary >= one_threshold)+  np.count_nonzero(y_pred_binary <= zero_threshold))\n",
    "\n",
    "    ################# timing the second code block #####################\n",
    "    start = time.time()\n",
    "\n",
    "    model.optimize()\n",
    "\n",
    "    end = time.time()\n",
    "    runtime2 = end - start\n",
    "    runtime = runtime1 + runtime2\n",
    "    ################## end #########################\n",
    "    \n",
    "    try:\n",
    "        print(\"Optimization time for model \", i, \": \", model.ObjVal)\n",
    "        print(\"Optimization value for model \", i, \": \", runtime)\n",
    "        opt_time_thres.append(runtime)\n",
    "        opt_val_thres.append(model.ObjVal)\n",
    "    except:\n",
    "        print(\"infeasible\")\n",
    "\n",
    "    \n",
    "    model.dispose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_dict_thres = {\n",
    "    \"opt_time\": opt_time_thres,\n",
    "    \"opt_val\": opt_val_thres,\n",
    "    \"opt_ones\": opt_ones_thres,\n",
    "    \"opt_utilized\": opt_utilized_thres\n",
    "}\n",
    "\n",
    "result_path = os.path.join(os.getcwd(), f\"Results\")\n",
    "with open(os.path.join(result_path, \"CNN_1D_opt_thres.pkl\"), 'wb') as f:\n",
    "    pickle.dump(opt_dict_thres, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(result_path, \"CNN_1D_opt_thres.pkl\"), 'rb') as f:\n",
    "    opt_dict_thres = pickle.load(f)\n",
    "\n",
    "opt_time_thres = opt_dict_thres[\"opt_time\"]\n",
    "opt_val_thres = opt_dict_thres[\"opt_val\"]\n",
    "opt_ones_thres = opt_dict_thres[\"opt_ones\"]\n",
    "opt_utilized_thres = opt_dict_thres[\"opt_utilized\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten opt_time\n",
    "opt_time_thres_mean = np.mean(opt_time_thres)\n",
    "print(\"Average optimization time: \", opt_time_thres_mean)\n",
    "# flatten opt_time\n",
    "opt_val_thres_mean = np.mean(opt_val_thres)\n",
    "print(\"Average optimization value: \", np.mean(opt_val_thres))\n",
    "print(\"Number of feasible model\", len(opt_time_thres))\n",
    "print(\"Number of ones: \", opt_ones_thres)\n",
    "print(\"Number of variable utilized \", opt_utilized_thres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing equality constraint with feasibility check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in paramaeters\n",
    "data_dir = os.path.join(os.getcwd(), 'systemData')\n",
    "EV_routes = pd.read_csv(os.path.join(data_dir, 'EV_routes.csv')).to_numpy()\n",
    "bus_params = pd.read_csv(os.path.join(data_dir, 'bus_params.csv')).to_numpy()\n",
    "# EV_schedules = pd.read_csv(os.path.join(data_dir, 'EV_schedules.csv')).to_numpy().astype(\"int32\")\n",
    "\n",
    "nbScen, nbTime, nbEV, nbBus, nbRoute = 5, 48, 5, bus_params.shape[0], EV_routes.shape[0]\n",
    "traffic = np.zeros(nbTime-1)\n",
    "traffic[14:20] = 1      # from 7-10am \n",
    "traffic[32:40] = 1      # from 4-8pm\n",
    "\n",
    "charging_station = [4, 8, 14, 20, 23, 28]\n",
    "non_charging_station = np.array([i for i in range(nbBus) if i not in charging_station])\n",
    "nbCS = len(charging_station)\n",
    "\n",
    "normal_nodes =  charging_station + list(range(101,108))\n",
    "virtual_nodes = list(range(201,205))\n",
    "congest_nodes = list(range(301,324))\n",
    "\n",
    "always_arc, normal_arc, congest_arc = [], [], []\n",
    "\n",
    "for r in range(nbRoute):\n",
    "    if (EV_routes[r,1] in (normal_nodes+virtual_nodes)) and (EV_routes[r,2] in (normal_nodes+virtual_nodes)) and (EV_routes[r,1] != EV_routes[r,2]):\n",
    "        normal_arc.append(r)\n",
    "    elif (EV_routes[r,1] in (normal_nodes+virtual_nodes)) and (EV_routes[r,2] in congest_nodes):\n",
    "        congest_arc.append(r)\n",
    "    else:\n",
    "        always_arc.append(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solution_check(y_pred_binary, model, index_test, EV_schedules):\n",
    "\n",
    "    variables = model.getVars()\n",
    "     \n",
    "    # ========================================= initialise parameters ===================================================\n",
    "\n",
    "    EVArcStatus = np.zeros((nbScen, nbEV, nbRoute, nbTime-1))\n",
    "    EVArcStatusIndex = np.zeros((nbScen, nbEV, nbRoute, nbTime-1))\n",
    "    EVChargeStatus = np.zeros((nbScen, nbEV, nbCS, nbTime))\n",
    "    EVChargeStatusIndex = np.zeros((nbScen, nbEV, nbCS, nbTime))\n",
    "    EVDischargeStatus = np.zeros((nbScen, nbEV, nbCS, nbTime))\n",
    "    EVDischargeStatusIndex = np.zeros((nbScen, nbEV, nbCS, nbTime))\n",
    "\n",
    "    for idx, x in enumerate(index_test):\n",
    "        name = variables[x].VarName\n",
    "        id = (re.search('\\[(.*)\\]', name).group(1)).split(\",\")\n",
    "        sc, k, i, s = int(id[0]), int(id[1]), int(id[2]), int(id[3])\n",
    "\n",
    "        if f\"EVArcStatus\" in name:\n",
    "            EVArcStatus[sc,k,i,s] = y_pred_binary[idx]\n",
    "            EVArcStatusIndex[sc,k,i,s] = x\n",
    "        elif f\"EVChargeStatus\" in name:\n",
    "            EVChargeStatus[sc,k,i,s] = y_pred_binary[idx]\n",
    "            EVChargeStatusIndex[sc,k,i,s] = x\n",
    "        elif f\"EVDischargeStatus\" in name:\n",
    "            EVDischargeStatus[sc,k,i,s] = y_pred_binary[idx]\n",
    "            EVDischargeStatusIndex[sc,k,i,s] = x\n",
    "\n",
    "    # ========================================= checking Arc constraints ===================================================\n",
    "    # set disable arc to 0 and check activated arc\n",
    "    for sc in range(nbScen):\n",
    "        for s in range(nbTime-1):\n",
    "            \n",
    "            activate_arc = (always_arc + congest_arc) if traffic[s] else (always_arc + normal_arc)\n",
    "            disable_arc = normal_arc if traffic[s] else congest_arc\n",
    "            \n",
    "            EVArcStatus[sc,:,np.array(disable_arc),s] = 0\n",
    "\n",
    "            # check for EVArcStatus sum == 1\n",
    "        \n",
    "        MoveStatus = np.sum(np.round(np.clip(EVArcStatus[sc], 0, None)), axis=1)\n",
    "        # print(MoveStatus)\n",
    "        for i in range(MoveStatus.shape[0]):\n",
    "            for j in range(MoveStatus.shape[1]):\n",
    "\n",
    "                activate_arc = (always_arc + congest_arc) if traffic[j] else (always_arc + normal_arc)\n",
    "\n",
    "                if MoveStatus[i,j] == 0:\n",
    "                    EVArcStatus[sc,i,activate_arc,j] = -1\n",
    "                elif MoveStatus[i,j] > 1:\n",
    "                    EVArcStatus[sc,i,:,j] = -1\n",
    "                    # max = np.argmax(EVArcStatus[i,:,j])\n",
    "                    # EVArcStatus[i,:,j] = 0\n",
    "                    # EVArcStatus[i,max,j] = 1\n",
    "\n",
    "        MoveStatus = np.sum(np.round(np.clip(EVArcStatus[sc], 0, None)), axis=1)\n",
    "        # print(MoveStatus)\n",
    "\n",
    "        # check EVArc Continuity\n",
    "        for k in range(nbEV):\n",
    "            for s in range(nbTime-2):\n",
    "                from_arc = np.max(np.round(EVArcStatus[sc,k,:,s]))\n",
    "                to_arc = np.max(np.round(EVArcStatus[sc,k,:,s+1]))\n",
    "\n",
    "                if from_arc == 1 and to_arc == 1:\n",
    "                    from_node = np.argmax(np.round(EVArcStatus[sc,k,:,s]))\n",
    "                    to_node = np.argmax(np.round(EVArcStatus[sc,k,:,s+1]))\n",
    "\n",
    "                    activate_arc = (always_arc + congest_arc) if traffic[s] else (always_arc + normal_arc)\n",
    "                    \n",
    "                    if EV_routes[from_node, 2] != EV_routes[to_node, 1]:\n",
    "                        EVArcStatus[sc,k,:,s] = -1\n",
    "                        EVArcStatus[sc,k,:,s+1] = -1\n",
    "                else:\n",
    "                    EVArcStatus[sc,k,:,s] = -1\n",
    "                    EVArcStatus[sc,k,:,s+1] = -1\n",
    "\n",
    "        # check schedule\n",
    "        nbSchedule = EV_schedules.shape[0]\n",
    "        for k in range(nbSchedule):\n",
    "            schedule = EV_schedules[k]  # [EV, destination, time]\n",
    "\n",
    "            route_index = (EV_routes[:,1]==schedule[1]).nonzero()[0] if schedule[2] == 0 else (EV_routes[:,2]==schedule[1]).nonzero()[0]\n",
    "            time = 0 if schedule[2] == 0 else schedule[2]-1\n",
    "\n",
    "            activate_arc = (always_arc + congest_arc) if traffic[time] else (always_arc + normal_arc)\n",
    "            disable_arc = normal_arc if traffic[time] else congest_arc\n",
    "\n",
    "            MoveStatus = np.sum(np.round(np.clip(EVArcStatus[sc,schedule[0], route_index, time], 0, None)))\n",
    "            if MoveStatus == 0:\n",
    "                EVArcStatus[sc,schedule[0],activate_arc,time] = 0\n",
    "                EVArcStatus[sc,schedule[0],route_index,time] = -1\n",
    "            elif MoveStatus > 1:\n",
    "                EVArcStatus[sc,schedule[0],activate_arc,time] = -1\n",
    "                # max = np.argmax(EVArcStatus[schedule[0],route_index,time])\n",
    "                # EVArcStatus[schedule[0],:,time] = 0\n",
    "                # EVArcStatus[schedule[0],max,time] = 1\n",
    "\n",
    "        # ========================================= checking charge constraints ===================================================\n",
    "        stationary_index = np.array((EV_routes[:,1] == EV_routes[:,2]).nonzero()[0])\n",
    "        charging_index = np.array([i for i, e in enumerate(EV_routes[:,1]) if e in charging_station])\n",
    "        charge_index = [i for i, e in enumerate(stationary_index) if e in charging_index]\n",
    "        for k in range(nbEV):\n",
    "            for i, ii in enumerate(stationary_index[charge_index]):\n",
    "                for s in range(nbTime-1):\n",
    "                    if not ((np.round(np.clip(EVChargeStatus[sc,k,i,s+1], 0, None)) + np.round(np.clip(EVDischargeStatus[sc,k,i,s+1], 0, None))) <= np.round(np.clip(EVArcStatus[sc,k,ii,s], 0, None))):\n",
    "                        EVChargeStatus[sc,k,i,s+1] = -1\n",
    "                        EVDischargeStatus[sc,k,i,s+1] = -1\n",
    "\n",
    "                        activate_arc = (always_arc + congest_arc) if traffic[s] else (always_arc + normal_arc)\n",
    "                        EVArcStatus[sc,k,:,s] = -1\n",
    "\n",
    "    # replacing y_predict\n",
    "    EVArcStatus = EVArcStatus.reshape(-1,)\n",
    "    EVArcStatusIndex = EVArcStatusIndex.reshape(-1,)\n",
    "    EVChargeStatus = EVChargeStatus.reshape(-1,)\n",
    "    EVChargeStatusIndex = EVChargeStatusIndex.reshape(-1,)\n",
    "    EVDischargeStatus = EVDischargeStatus.reshape(-1,)\n",
    "    EVDischargeStatusIndex = EVDischargeStatusIndex.reshape(-1,)\n",
    "\n",
    "    corrected_y = np.concatenate((EVArcStatus, EVChargeStatus, EVDischargeStatus))\n",
    "    corrected_idx = np.concatenate((EVArcStatusIndex, EVChargeStatusIndex, EVDischargeStatusIndex)).astype(\"int32\")\n",
    "\n",
    "    return corrected_y, corrected_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "gurobi_env = gb.Env()\n",
    "gurobi_env.setParam(\"OutputFlag\", 0)\n",
    "\n",
    "# loop through all test models and calculate average optimization time\n",
    "opt_time_feas = []\n",
    "opt_val_feas = []\n",
    "opt_ones_feas = []\n",
    "opt_utilized_feas = []\n",
    "for i, x in enumerate(model_test):\n",
    "    # if i == 40:\n",
    "\n",
    "    path = os.path.join(os.getcwd(), \"dataGeneration/model\")\n",
    "\n",
    "    model = gb.read(os.path.join(path, f\"coordination_{x}.mps\"), env=gurobi_env)\n",
    "    model.setParam(\"OutputFlag\", 0)\n",
    "    model.setParam(\"TimeLimit\", 5*60)\n",
    "\n",
    "    ################# timing the first code block #####################\n",
    "    start = time.time()\n",
    "\n",
    "    # deep learning prediciton\n",
    "    inputs = torch.tensor(np.expand_dims(test_dataset.X[i], axis=0), dtype=torch.float32) \n",
    "    inputs = inputs.to(device)    \n",
    "    outputs = net(inputs) \n",
    "\n",
    "    ################## end #########################\n",
    "\n",
    "    y_pred_binary =  (outputs).reshape(-1,).cpu().detach().numpy()\n",
    "    # y_pred_binary = y_test[i]\n",
    "\n",
    "    modelVars = model.getVars()\n",
    "\n",
    "    one_threshold = (np.mean(mean_one))\n",
    "    zero_threshold = (1-np.mean(mean_zero))\n",
    "    # one_threshold = ((1-np.mean(one_accuracy)) * 0.5) + 0.5\n",
    "    # zero_threshold = 0.5 - ((1-np.mean(zero_accuracy)) * 0.5)\n",
    "    # one_threshold = 0.74\n",
    "    # zero_threshold = 0.3\n",
    "\n",
    "    # use equality constraint from here on\n",
    "    for j in range(len(y_pred_binary)):\n",
    "        if (y_pred_binary[j] >= one_threshold or y_pred_binary[j] <= zero_threshold):\n",
    "            modelVars[index_test[i][j]].setAttr(\"LB\", round(y_pred_binary[j]))\n",
    "            modelVars[index_test[i][j]].setAttr(\"UB\", round(y_pred_binary[j]))\n",
    "            \n",
    "    end = time.time()\n",
    "    runtime1 = end - start\n",
    "\n",
    "    opt_ones_feas.append(np.count_nonzero(y_pred_binary >= one_threshold))\n",
    "    opt_utilized_feas.append(np.count_nonzero(y_pred_binary >= one_threshold)+  np.count_nonzero(y_pred_binary <= zero_threshold))\n",
    "\n",
    "    ################# timing the second code block #####################\n",
    "    start = time.time()\n",
    "\n",
    "    model.optimize()\n",
    "\n",
    "    end = time.time()\n",
    "    runtime2 = end - start\n",
    "    runtime = runtime1 + runtime2\n",
    "    ################## end #########################\n",
    "    \n",
    "    try:\n",
    "        print(\"Optimization time for model \", i, \": \", model.ObjVal)\n",
    "        print(\"Optimization value for model \", i, \": \", runtime)\n",
    "        opt_time_feas.append(runtime)\n",
    "        opt_val_feas.append(model.ObjVal)\n",
    "    except:\n",
    "        print(\"infeasible, reoptimizing...\")\n",
    "\n",
    "        model.dispose()\n",
    "\n",
    "        model = gb.read(os.path.join(path, f\"coordination_{x}.mps\"), env=gurobi_env)\n",
    "        model.setParam(\"OutputFlag\", 0)\n",
    "        model.setParam(\"TimeLimit\", 5*60)\n",
    "\n",
    "        modelVars = model.getVars()\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        # one_threshold = one_threshold+0.1\n",
    "        # new_index = index_test[i]\n",
    "        y_pred_binary =  (outputs).reshape(-1,).cpu().detach().numpy()\n",
    "        y_pred_binary = np.where((y_pred_binary >= zero_threshold) & (y_pred_binary <= one_threshold), -1, np.round(y_pred_binary))\n",
    "        y_pred_binary, new_index = solution_check(y_pred_binary, model, index_test[i], schedule_test[i])\n",
    "\n",
    "        for j in range(len(y_pred_binary)):\n",
    "            if (y_pred_binary[j] >= 0):\n",
    "                modelVars[new_index[j]].setAttr(\"LB\", round(y_pred_binary[j]))\n",
    "                modelVars[new_index[j]].setAttr(\"UB\", round(y_pred_binary[j]))\n",
    "            \n",
    "        end = time.time()\n",
    "        runtime3 = end - start\n",
    "\n",
    "        opt_ones_feas.append(np.count_nonzero(y_pred_binary >= one_threshold))\n",
    "        opt_utilized_feas.append(np.count_nonzero(y_pred_binary >= one_threshold)+  np.count_nonzero(y_pred_binary <= zero_threshold))\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        model.optimize()\n",
    "\n",
    "        end = time.time()\n",
    "        runtime4 = end - start\n",
    "        runtime = runtime + runtime3 + runtime4\n",
    "\n",
    "        try:\n",
    "            print(\"Optimization time for model \", i, \": \", model.ObjVal)\n",
    "            print(\"Optimization value for model \", i, \": \", runtime)\n",
    "            opt_time_feas.append(runtime)\n",
    "            opt_val_feas.append(model.ObjVal)\n",
    "        except:\n",
    "            print(\"infeasible\")\n",
    "\n",
    "            # model.computeIIS()\n",
    "\n",
    "            # for c in model.getConstrs():\n",
    "            #     if c.IISConstr: print(f'\\t{c.constrname}: {model.getRow(c)} {c.Sense} {c.RHS}')\n",
    "\n",
    "            # for v in model.getVars():\n",
    "            #     if v.IISLB: print(f'\\t{v.varname} ≥ {v.LB}')\n",
    "            #     if v.IISUB: print(f'\\t{v.varname} ≤ {v.UB}')\n",
    "    \n",
    "    model.dispose()\n",
    "        # break\n",
    "    # else:\n",
    "    #     print(\"skip\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_dict_feas = {\n",
    "    \"opt_time\": opt_time_feas,\n",
    "    \"opt_val\": opt_val_feas,\n",
    "    \"opt_ones\": opt_ones_feas,\n",
    "    \"opt_utilized\": opt_utilized_feas\n",
    "}\n",
    "\n",
    "result_path = os.path.join(os.getcwd(), f\"Results\")\n",
    "with open(os.path.join(result_path, \"CNN_1D_opt_feas.pkl\"), 'wb') as f:\n",
    "    pickle.dump(opt_dict_feas, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(result_path, \"CNN_1D_opt_feas.pkl\"), 'rb') as f:\n",
    "    opt_dict_feas = pickle.load(f)\n",
    "\n",
    "opt_time_feas = opt_dict_feas[\"opt_time\"]\n",
    "opt_val_feas = opt_dict_feas[\"opt_val\"]\n",
    "opt_ones_feas = opt_dict_feas[\"opt_ones\"]\n",
    "opt_utilized_feas = opt_dict_feas[\"opt_utilized\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten opt_time\n",
    "opt_time_feas_mean = np.mean(opt_time_feas)\n",
    "print(\"Average optimization time: \", np.mean(opt_time_feas))\n",
    "# flatten opt_time\n",
    "opt_val_feas_mean = np.mean(opt_val_feas)\n",
    "print(\"Average optimization value: \", np.mean(opt_val_feas))\n",
    "print(\"Number of feasible model\", len(opt_time_feas))\n",
    "print(\"Number of ones: \", opt_ones_feas)\n",
    "print(\"Number of variable utilized \", opt_utilized_feas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate time sped up by and optimality difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speedup_time_thres = 1 - (opt_time_thres_mean/opt_time_baseline)\n",
    "optimality_loss_thres = (opt_val_thres_mean - opt_val_baseline) / opt_val_baseline\n",
    "feasible_model_thres = len(opt_time_thres) / len(model_test) * 100\n",
    "# ones_utilized_thres = np.array(opt_ones_thres) / np.array(opt_ones) *100\n",
    "# prediction_utilized_thres = np.array(opt_utilized_thres) / len(index_test[0]) *100\n",
    "\n",
    "speedup_time_feas = 1 - (opt_time_feas_mean/opt_time_baseline)\n",
    "optimality_loss_feas = (opt_val_feas_mean - opt_val_baseline) / opt_val_baseline\n",
    "feasible_model_feas = len(opt_time_feas) / len(model_test) * 100\n",
    "# ones_utilized_feas = np.array(opt_ones_feas) / np.array(opt_ones) *100\n",
    "# prediction_utilized_feas = np.array(opt_utilized_feas) / len(index_test[0]) *100\n",
    "\n",
    "print(f\"Time sped up for threshold & feasibility check: {speedup_time_thres*100} %, {speedup_time_feas*100} %\")\n",
    "print(f\"Optimality Loss for threshold & feasibility cheack: {optimality_loss_thres*100} %, {optimality_loss_feas*100} %\")\n",
    "print(f\"Feasible model for threshold & feasibility cheack: {feasible_model_thres} %, {feasible_model_feas} %\")\n",
    "# print(f\"Average Number ones used for threshold & feasibility cheack: {np.mean(ones_utilized_thres)} %, {np.mean(ones_utilized_feas)} %\")\n",
    "# print(f\"Number of prediction utilized for threshold & feasibility cheack: {np.mean(prediction_utilized_thres)} %, {np.mean(prediction_utilized_feas)} %\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
